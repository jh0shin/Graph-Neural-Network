>>>>>> import torch
>>>>>> from . import _functional as F
>>>>>> from .optimizer import Optimizer
       
       
>>>>>> class Adam(Optimizer):
           r"""Implements Adam algorithm.
       
           It has been proposed in `Adam: A Method for Stochastic Optimization`_.
           The implementation of the L2 penalty follows changes proposed in
           `Decoupled Weight Decay Regularization`_.
       
           Args:
               params (iterable): iterable of parameters to optimize or dicts defining
                   parameter groups
               lr (float, optional): learning rate (default: 1e-3)
               betas (Tuple[float, float], optional): coefficients used for computing
                   running averages of gradient and its square (default: (0.9, 0.999))
               eps (float, optional): term added to the denominator to improve
                   numerical stability (default: 1e-8)
               weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
               amsgrad (boolean, optional): whether to use the AMSGrad variant of this
                   algorithm from the paper `On the Convergence of Adam and Beyond`_
                   (default: False)
       
           .. _Adam\: A Method for Stochastic Optimization:
               https://arxiv.org/abs/1412.6980
           .. _Decoupled Weight Decay Regularization:
               https://arxiv.org/abs/1711.05101
           .. _On the Convergence of Adam and Beyond:
               https://openreview.net/forum?id=ryQu7f-RZ
           """
       
>>>>>>     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                        weight_decay=0, amsgrad=False):
    1:         if not 0.0 <= lr:
>>>>>>             raise ValueError("Invalid learning rate: {}".format(lr))
    1:         if not 0.0 <= eps:
>>>>>>             raise ValueError("Invalid epsilon value: {}".format(eps))
    1:         if not 0.0 <= betas[0] < 1.0:
>>>>>>             raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
    1:         if not 0.0 <= betas[1] < 1.0:
>>>>>>             raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
    1:         if not 0.0 <= weight_decay:
>>>>>>             raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
    2:         defaults = dict(lr=lr, betas=betas, eps=eps,
    1:                         weight_decay=weight_decay, amsgrad=amsgrad)
    1:         super(Adam, self).__init__(params, defaults)
       
>>>>>>     def __setstate__(self, state):
>>>>>>         super(Adam, self).__setstate__(state)
>>>>>>         for group in self.param_groups:
>>>>>>             group.setdefault('amsgrad', False)
       
>>>>>>     @torch.no_grad()
>>>>>>     def step(self, closure=None):
               """Performs a single optimization step.
       
               Args:
                   closure (callable, optional): A closure that reevaluates the model
                       and returns the loss.
               """
    1:         loss = None
    1:         if closure is not None:
>>>>>>             with torch.enable_grad():
>>>>>>                 loss = closure()
       
    2:         for group in self.param_groups:
    1:             params_with_grad = []
    1:             grads = []
    1:             exp_avgs = []
    1:             exp_avg_sqs = []
    1:             state_sums = []
    1:             max_exp_avg_sqs = []
    1:             state_steps = []
       
    5:             for p in group['params']:
    4:                 if p.grad is not None:
    4:                     params_with_grad.append(p)
    4:                     if p.grad.is_sparse:
>>>>>>                         raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
    4:                     grads.append(p.grad)
       
    4:                     state = self.state[p]
                           # Lazy state initialization
    4:                     if len(state) == 0:
    4:                         state['step'] = 0
                               # Exponential moving average of gradient values
    4:                         state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                               # Exponential moving average of squared gradient values
    4:                         state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
    4:                         if group['amsgrad']:
                                   # Maintains max of all exp. moving avg. of sq. grad. values
>>>>>>                             state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
       
    4:                     exp_avgs.append(state['exp_avg'])
    4:                     exp_avg_sqs.append(state['exp_avg_sq'])
       
    4:                     if group['amsgrad']:
>>>>>>                         max_exp_avg_sqs.append(state['max_exp_avg_sq'])
       
                           # update the steps for each param group update
    4:                     state['step'] += 1
                           # record the step after step update
    4:                     state_steps.append(state['step'])
       
    1:             beta1, beta2 = group['betas']
    2:             F.adam(params_with_grad,
    1:                    grads,
    1:                    exp_avgs,
    1:                    exp_avg_sqs,
    1:                    max_exp_avg_sqs,
    1:                    state_steps,
    1:                    group['amsgrad'],
    1:                    beta1,
    1:                    beta2,
    1:                    group['lr'],
    1:                    group['weight_decay'],
    1:                    group['eps'])
    1:         return loss
